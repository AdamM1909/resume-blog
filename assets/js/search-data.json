{
  
    
        "post0": {
            "title": "My First Kaggle Competition  Football Match Probability Prediction",
            "content": "My First Kaggle Competition - Football Match Probability Prediction . This blog post will cover the main takeaways from my first kaggle competition. It will be broken down into the following sections: . A brief overview of the competition, my approach, and my score. . | . Things I wished I’d known before starting. . | . Importance of understanding the dataset + thoughtful feature engineering. . | . The models I used and their performance. . | . Places to improve and my next steps. . | . A brief overview of the competition, my approach, and my score . The aim of this competition was to predict the match outcome of 189 games of football using statistics from the team’s last ten games. . My general approach was to extract short, medium, and long term averages of different features included in or engineered from the original data set. I used feature importance from a decision tree forest to see if these signals had good predictive power or not before ensembling a few different kinds of models for a final submission. . The predictions were judged using a log-loss score so overconfidence was punished. I scored a respectable 1.00520. For context, the leader scored 0.98834, and the bookies 0.9730. My real aim for this competition was to get used to the end-to-end process of predictive modelling and to this means it was a success. . Here’s the screengrab which, at the time of writing, put me in 51st out of 288 position: . . Things I wished I’d known before starting . I’ll put these in a list for ease: . Once testing is complete, train a final model with the whole data set provided - not just the training split. This hampered my progress quite a lot before I noticed but it was a valuable learning point nonetheless. . | Don’t be afraid to do some hard coding. By this I mean looping through variable names caused complications and lead to mistakes. If in doubt just copy and paste a few times to be safe. . | Use box and whisker plots or histograms after each newly added feature to check things went as planned. . | Importance of understanding the dataset + thoughtful feature engineering . Most of my gains didn’t come from the actual model I chose - in fact I hardly optimised these at all - they came from deriving useful metrics from the data and getting rid of useless ones. . By looking at how each feature split in a tree improves the model’s predictive power, feature importance can be gauged. Here’s a bar chart of my top fifteen features: . . Most of the variables are fairly self explanatory; however, I’ll add that ‘dif’ refers to a difference between the home and the away team; ‘1’,’2’,’3’ and ‘long’,’med’,’short’ refer to the respective time average taken; and ‘s/c’ is the difference between scored and conceded goals. . This shows how explicitly feeding the model differences really helps it categorise games. I was most surprised to see how ‘rest_dif_long’, which is the long term average rest differences between games between the home and away team, is judged as fairly important by the decision trees. . Team ratings were provided by Ocotosport, and again the relative difference between the averages proved most useful. . I didn’t apply any explicit time series transforms to this data but I’d be interested to see how decomposing ratings trends into signal components with a fourier transform of some kind may provide extra signal - having said this, extra complexity always comes at a price. . The models I used and their performance . My final submission was an ensemble of a logistic regression model; a random forest mode; a light gradient boosted model; and a two layer neural network. Okay this was overkill but it was more for learning the strengths of different models - ensembling was just an easy extra kick up the leaderboard. . Surprisingly simple a logistic model alone would have scored very well. This speaks a lot for simple models and is definitely a good take away: start simple, the rest is limiting returns. . The shallow neural net extrapolated really well out of its training batch. I used fast.ai’s tabular model, which incorporates lots of under the hood tricks to work well on tabular data. Their high level API helped to split categorical and continuous features; make the entity embeddings for categorical features; and find a good learning rate for a learning rate varied fitting cycle. . Places to improve and my next steps . The glaring area for further research is running a grid search on my tree based models. To do this I think I’ll have to rent a better GPU as the free version on Paperspace simply took too long. . Having said this, for a first attempt, I’m happy with my performance as I picked up the main points about making a good model. Optimising little things is the aim of the game of Kaggle competitions but for me it’s about learning as much as I can. . My plan now is to learn some more about natural language processing models and do a deep dive into the maths behind. .",
            "url": "https://adamm1909.github.io/resume-blog/2022/04/23/My-First-Kaggle-Competition-Football-Match-Probability-Prediction.html",
            "relUrl": "/2022/04/23/My-First-Kaggle-Competition-Football-Match-Probability-Prediction.html",
            "date": " • Apr 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Grasping The Basics Of Machine Learning",
            "content": "Grasping The Basics of Machine Learning . The technicalities of many machine learning techniques are profoundly difficult to understand , let alone to successfully implement. Sometimes implementation can be achieved without understanding the basics; other times the reverse is true. Either way, it is rarely smooth sailing to design a predictive model from start to finish as self-learning tutorials seem to imply. . I’ve studied the process a fair bit now and, having just completed fast.ai’s practical deep learning for coders, I thought this would be an ideal time to summarise my current knowledge of machine learning and how I’ve learnt it. . I’ve this blog down into the following short categories: . Overall top down approach to learning . | Using simple models to clean data . | Base rates . | A general implementation strategy . | My next steps - Kaggle . | . Overall Top Down Approach To Learning . No doubt there will be many sharp minded non computer science graduates intrigued by the world of machine learning being met with a barrage of jargon and technical smoke and mirrors. For some they can push through. For most it’s enough to quit and leave feeling quite defeated. . Top down learning is a must. Starting with fairly basic Python skills and someone else’s fully implemented model is the best way I’ve found to pick up skills in this field. Gradually, natural questing leads you beneath the black box to the source code, the Arxiv papers and the Google dev talks. . Using Simple Models To Clean Data . This is the first of the more technical chapters of this blog. . Below is an image which a galaxy classifier I made struggled to label. My calissifier was a restrained version of Resnet 50 and I built a GUI to look at its least confident predictions and remove them from the training set if I thought they were inappropriate for the model. . With this output, it’s easy to spot that one photo isn’t an actual galaxy but a clipart-like drawing so it shouldn’t be used for training my model. . This post-evaluation method sounds logical but wasn’t my immediate approach to cleaning up data. . I’ve applied it elsewhere with simple decision trees to see if a feature is important before passing the reduced data to a neural net. I think it’s a good trick to have in the bag, and often is a quick way to get rid of dodgy data. . . Using Base Rates . This chapter can be summarised in one question: is the model beating a rough and ready statistical/random guess approach? . This is akin to checking a velocity is less than the speed of light in physics. A vital sanity check which is often overlooked. . My General Implement Strategy . 1) &gt; Load the data into a Jupyter notebook and state some exploratory data analysis to include as a minimum: . - &gt; Box plots - &gt; Histograms - &gt; Correlation Matrices . 2) &gt; Cut Cut Cut. Take away as much complexity as possible in the initial stages using the exploratory data analysis and the simplest model for the job. This idea is built around a framework outlined by Elon Musk. There’s no point optimising a complex data set/model if parts of it are unnecessary for the job in hand. . 3) &gt; Does the simple model beat an appropriate base rate test ? . 4) &gt; Feature engineering. Can any features be merged, can images be transformed etc. This is a good way of providing the model with more high quality data. . 5) &gt; Test a range of models (random forest, neural net, gradient boosting) to get an idea for which one might be most appropriate. . Now for the parts I’m still working on… . 6) &gt; Optimising the model. Use grid search to tune hyperparameters to get the most out of the model. Consider paying for a more powerful GPU. . 7) &gt; Transfer ideas from recently published papers to your work. One example I have successfully implemented is the learning rate finder and learning rate cycle. . This leads nicely onto my final paragraph. . My next steps - Implementing state of the art approaches in Kaggle Competitions . I’m aiming to score in the top half in this football probability prediction challenge on kaggle. I’ll write a new blog on that when I have progress. .",
            "url": "https://adamm1909.github.io/resume-blog/2022/04/21/Grasping-The-Basics-of-Machine-Learning.html",
            "relUrl": "/2022/04/21/Grasping-The-Basics-of-Machine-Learning.html",
            "date": " • Apr 21, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "ML CV",
          "content": "Courses . Neural Networks and Deep Learning . Practical Deep Learning For Coders . Pratcicle Deep Learning For Coders Part 2: Deep Learning from the Foundations (work in progress) . Papers . Deep Prediction of Investor Interest: A Supervised Clustering Approach . Unsupervised Data Augmentation for Consistencey Training . Competitions . Football Match Probabilty Prediction .",
          "url": "https://adamm1909.github.io/resume-blog/CV/",
          "relUrl": "/CV/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "My name is Adam Myers. I’m currently a University of Manchester Therotical Physics undergraduate self-teaching the fundamentals of machine learning. I have a good mathematical stating point but most of what I cover on this blog will be new to me. . Specifically, this blog outlines how and what im learning. It includes content both designed to help people starting out and to showcase my understadning of the field. . I hope to present some sucessful Kaggle entries in the near future to showcase my progress. .",
          "url": "https://adamm1909.github.io/resume-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adamm1909.github.io/resume-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}